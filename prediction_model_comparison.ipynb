{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the AUROC and AUPRC results for plotting\n",
    "list_dict = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to track CPU time and Memory usage\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "start_memory = psutil.virtual_memory().available\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.COVID-19 graph embedding alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, average_precision_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules import Module\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='data/'\n",
    "exp_id='v0'\n",
    "device_id='cpu' #'cpu' if CPU, device number if GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=pickle.load(open(data_path+'LabelEncoder_'+exp_id+'.pkl', 'rb'))\n",
    "edge_index=pickle.load(open(data_path+'edge_index_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "types=np.array([item.split('_')[0] for item in le.classes_ ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label\n",
    "trials=pd.read_excel(data_path+'literature-mining/All_trails_5_24.xlsx',header=1,index_col=0)\n",
    "trials_drug=set([drug.strip().upper() for lst in trials.loc[trials['study_category'].apply(lambda x: 'drug' in x.lower()),'intervention'].apply(lambda x: re.split(r'[+|/|,]',x.replace(' vs. ', '/').replace(' vs ', '/').replace(' or ', '/').replace(' with and without ', '/').replace(' /wo ', '/').replace(' /w ', '/').replace(' and ', '/').replace(' - ', '/').replace(' (', '/').replace(') ', '/'))).values for drug in lst])\n",
    "drug_labels=[1 if drug.split('_')[1] in trials_drug else 0 for drug in le.classes_[types=='drug'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self,embedding_dim):\n",
    "        super(Classifier, self).__init__() \n",
    "        self.fc1=nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.fc2=nn.Linear(embedding_dim,1)\n",
    "        self.bn=nn.BatchNorm1d(embedding_dim)\n",
    "    def forward(self, x):\n",
    "        residual1 = x\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x= self.bn(F.dropout(F.relu(self.fc1(x)),training=self.training))\n",
    "        x += residual1  \n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import BatchSampler, WeightedRandomSampler\n",
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self, num_neg_samples):\n",
    "        super(BPRLoss, self).__init__()\n",
    "        self.num_neg_samples=num_neg_samples\n",
    "    \n",
    "    def forward(self, output, label):\n",
    "        positive_output=output[label==1]\n",
    "        negative_output=output[label!=1]\n",
    "        \n",
    "        #negative sample proportional to the high values\n",
    "        negative_sampler=WeightedRandomSampler(negative_output-min(negative_output), num_samples=self.num_neg_samples*len(positive_output),replacement=True)\n",
    "        negative_sample_output=negative_output[torch.tensor(list(BatchSampler(negative_sampler, batch_size=len(positive_output),drop_last=True)),dtype=torch.long).t()]\n",
    "        return -(positive_output.view(-1,1)-negative_sample_output).sigmoid().log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_np = pickle.load(open(data_path+'COVID_embedding_'+exp_id+'.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.8103823661804199\n",
      "test loss 0.6932399272918701\n",
      "training loss 0.8063710927963257\n",
      "test loss 0.6932365298271179\n",
      "training loss 0.8113760948181152\n",
      "test loss 0.6932251453399658\n",
      "training loss 0.8406867980957031\n",
      "test loss 0.6931976079940796\n",
      "training loss 0.7960447669029236\n",
      "test loss 0.6932141184806824\n",
      "training loss 0.7339330315589905\n",
      "test loss 0.6932159662246704\n",
      "training loss 0.7969534397125244\n",
      "test loss 0.6932098269462585\n",
      "training loss 0.7890408635139465\n",
      "test loss 0.693217933177948\n",
      "training loss 0.9003066420555115\n",
      "test loss 0.6931884288787842\n",
      "training loss 0.822611391544342\n",
      "test loss 0.6931594610214233\n",
      "training loss 0.8152236342430115\n",
      "test loss 0.693177342414856\n",
      "training loss 0.8225076198577881\n",
      "test loss 0.6930989623069763\n",
      "training loss 0.728827714920044\n",
      "test loss 0.69306880235672\n",
      "training loss 0.7472310066223145\n",
      "test loss 0.6930496692657471\n",
      "training loss 0.7640727162361145\n",
      "test loss 0.6930521726608276\n",
      "training loss 0.7679376006126404\n",
      "test loss 0.6929921507835388\n",
      "training loss 0.7724018096923828\n",
      "test loss 0.6929621696472168\n",
      "training loss 0.7636822462081909\n",
      "test loss 0.6929010152816772\n",
      "training loss 0.7913829684257507\n",
      "test loss 0.6929148435592651\n",
      "training loss 0.7507764101028442\n",
      "test loss 0.6928204298019409\n",
      "training loss 0.8413498997688293\n",
      "test loss 0.6928063631057739\n",
      "training loss 0.7603559494018555\n",
      "test loss 0.6926935315132141\n",
      "training loss 0.7925270199775696\n",
      "test loss 0.6927602291107178\n",
      "training loss 0.7458914518356323\n",
      "test loss 0.692708432674408\n",
      "training loss 0.742565929889679\n",
      "test loss 0.6926616430282593\n",
      "training loss 0.7625687122344971\n",
      "test loss 0.692588210105896\n",
      "training loss 0.7600193023681641\n",
      "test loss 0.6926112174987793\n",
      "training loss 0.760430634021759\n",
      "test loss 0.692445695400238\n",
      "training loss 0.818477988243103\n",
      "test loss 0.6925230026245117\n",
      "training loss 0.7284418344497681\n",
      "test loss 0.6924964785575867\n"
     ]
    }
   ],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+'nn_clf_covid_embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+'nn_clf_covid_embedding.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.6552501214943016\n",
      "AUPRC 0.08606489606331079\n"
     ]
    }
   ],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "auroc = metrics.roc_auc_score(y_test,prob)\n",
    "auprc = metrics.average_precision_score(y_test,prob)\n",
    "print(\"AUROC\", auroc)\n",
    "print(\"AUPRC\", auprc)\n",
    "\n",
    "list_dict.append({\n",
    "    'embed_method': 'COVID-19 alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'neural network ranking',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 3.91 sec\n",
      "Memory usage: 44.25 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.6931524243991911\n",
      "Logit AUPRC 0.10168666277003269\n",
      "XGBoost AUROC 0.6519384219849818\n",
      "XGBoost AUPRC 0.08424654473848554\n",
      "rf AUROC 0.5822438037906222\n",
      "rf AUPRC 0.0665767657030504\n",
      "svm AUROC 0.6586402044239603\n",
      "svm AUPRC 0.1319144480743561\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"Logit AUROC\", auroc)\n",
    "print(\"Logit AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'COVID-19 alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'logistic regression',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"XGBoost AUROC\", auroc)\n",
    "print(\"XGBoost AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'COVID-19 alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'XGBoost',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"rf AUROC\", auroc)\n",
    "print(\"rf AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'COVID-19 alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'random forest',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"svm AUROC\", auroc)\n",
    "print(\"svm AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'COVID-19 alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'support vector machines',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 7.05 sec\n",
      "Memory usage: -10.67 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DRKG embedding alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_np = pickle.load(open(data_path+'node_feature_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 400\n",
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.8182557225227356\n",
      "test loss 0.5654714703559875\n",
      "training loss 0.48486700654029846\n",
      "test loss 0.504222571849823\n",
      "training loss 0.29547011852264404\n",
      "test loss 0.48464715480804443\n",
      "training loss 0.3150358498096466\n",
      "test loss 0.498331755399704\n",
      "training loss 0.2940555810928345\n",
      "test loss 0.497141033411026\n",
      "training loss 0.28010106086730957\n",
      "test loss 0.49028658866882324\n",
      "training loss 0.2777594327926636\n",
      "test loss 0.4876006245613098\n",
      "training loss 0.21368248760700226\n",
      "test loss 0.5224721431732178\n",
      "training loss 0.28686225414276123\n",
      "test loss 0.519257664680481\n",
      "training loss 0.2793280780315399\n",
      "test loss 0.5361437797546387\n",
      "training loss 0.21720975637435913\n",
      "test loss 0.5233345031738281\n",
      "training loss 0.25004202127456665\n",
      "test loss 0.5403933525085449\n",
      "training loss 0.1824733018875122\n",
      "test loss 0.5294878482818604\n",
      "training loss 0.19845442473888397\n",
      "test loss 0.5399629473686218\n",
      "training loss 0.27991464734077454\n",
      "test loss 0.5361773371696472\n",
      "training loss 0.2212509959936142\n",
      "test loss 0.521087646484375\n",
      "training loss 0.22415423393249512\n",
      "test loss 0.5380112528800964\n",
      "training loss 0.19264698028564453\n",
      "test loss 0.5500249266624451\n",
      "training loss 0.17568430304527283\n",
      "test loss 0.5141462087631226\n",
      "training loss 0.17209786176681519\n",
      "test loss 0.5361774563789368\n",
      "training loss 0.23069390654563904\n",
      "test loss 0.5378098487854004\n",
      "training loss 0.14344219863414764\n",
      "test loss 0.48007282614707947\n",
      "training loss 0.2426915168762207\n",
      "test loss 0.5558348894119263\n",
      "training loss 0.18957677483558655\n",
      "test loss 0.5125201940536499\n",
      "training loss 0.18432950973510742\n",
      "test loss 0.5237368941307068\n",
      "training loss 0.17143549025058746\n",
      "test loss 0.5021042227745056\n",
      "training loss 0.21560387313365936\n",
      "test loss 0.4862250089645386\n",
      "training loss 0.16393761336803436\n",
      "test loss 0.4398695230484009\n",
      "training loss 0.1811019778251648\n",
      "test loss 0.47268110513687134\n",
      "training loss 0.14336740970611572\n",
      "test loss 0.46910953521728516\n"
     ]
    }
   ],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+'nn_clf_DRKG_embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+'nn_clf_DRKG_embedding.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.8657762310116166\n",
      "AUPRC 0.17444598694313124\n"
     ]
    }
   ],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "auroc = metrics.roc_auc_score(y_test,prob)\n",
    "auprc = metrics.average_precision_score(y_test,prob)\n",
    "print(\"AUROC\", auroc)\n",
    "print(\"AUPRC\", auprc)\n",
    "\n",
    "list_dict.append({\n",
    "    'embed_method': 'DRKG alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'neural network ranking',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 2.63 sec\n",
      "Memory usage: 12.00 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.8436015613977331\n",
      "Logit AUPRC 0.18394788718419244\n",
      "XGBoost AUROC 0.8578908589255201\n",
      "XGBoost AUPRC 0.14256704270151688\n",
      "rf AUROC 0.8432174826380725\n",
      "rf AUPRC 0.13306874486326117\n",
      "svm AUROC 0.8842276881594006\n",
      "svm AUPRC 0.2578582442037748\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"Logit AUROC\", auroc)\n",
    "print(\"Logit AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'DRKG alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'logistic regression',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"XGBoost AUROC\", auroc)\n",
    "print(\"XGBoost AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'DRKG alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'XGBoost',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"rf AUROC\", auroc)\n",
    "print(\"rf AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'DRKG alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'random forest',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"svm AUROC\", auroc)\n",
    "print(\"svm AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'DRKG alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'support vector machines',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 5.87 sec\n",
      "Memory usage: -0.17 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. hybrid embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. embedding dimension: 128 (default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "z_np = pickle.load(open(data_path+f'hybrid_embedding_{embed_dim}_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 1.0323024988174438\n",
      "test loss 0.7024515271186829\n",
      "training loss 0.8107777833938599\n",
      "test loss 0.6513453125953674\n",
      "training loss 0.6297714114189148\n",
      "test loss 0.6260812878608704\n",
      "training loss 0.5238948464393616\n",
      "test loss 0.6047687530517578\n",
      "training loss 0.4022414982318878\n",
      "test loss 0.5926344990730286\n",
      "training loss 0.39312437176704407\n",
      "test loss 0.6003910899162292\n",
      "training loss 0.3976527750492096\n",
      "test loss 0.6000576615333557\n",
      "training loss 0.32615718245506287\n",
      "test loss 0.6163175106048584\n",
      "training loss 0.39616668224334717\n",
      "test loss 0.6290398240089417\n",
      "training loss 0.3855721652507782\n",
      "test loss 0.6002631187438965\n",
      "training loss 0.3514969050884247\n",
      "test loss 0.6139815449714661\n",
      "training loss 0.38395676016807556\n",
      "test loss 0.6186772584915161\n",
      "training loss 0.4060250222682953\n",
      "test loss 0.6627027988433838\n",
      "training loss 0.3827492892742157\n",
      "test loss 0.6590971350669861\n",
      "training loss 0.38731563091278076\n",
      "test loss 0.6719714999198914\n",
      "training loss 0.4262847304344177\n",
      "test loss 0.6633058786392212\n",
      "training loss 0.4129684865474701\n",
      "test loss 0.6726946234703064\n",
      "training loss 0.41208401322364807\n",
      "test loss 0.6959264278411865\n",
      "training loss 0.4950030744075775\n",
      "test loss 0.6599030494689941\n",
      "training loss 0.3954767882823944\n",
      "test loss 0.6890814900398254\n",
      "training loss 0.35250186920166016\n",
      "test loss 0.6927457451820374\n",
      "training loss 0.36388957500457764\n",
      "test loss 0.6832172870635986\n",
      "training loss 0.3548441231250763\n",
      "test loss 0.7134066820144653\n",
      "training loss 0.3324948847293854\n",
      "test loss 0.7157419919967651\n",
      "training loss 0.4019436538219452\n",
      "test loss 0.7100571990013123\n",
      "training loss 0.3802124559879303\n",
      "test loss 0.7225906252861023\n",
      "training loss 0.3892151415348053\n",
      "test loss 0.7359848618507385\n",
      "training loss 0.3670562207698822\n",
      "test loss 0.7202051281929016\n",
      "training loss 0.387797087430954\n",
      "test loss 0.7377002239227295\n",
      "training loss 0.3692079484462738\n",
      "test loss 0.6944783926010132\n"
     ]
    }
   ],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.8915291037639719\n",
      "AUPRC 0.24000344741010085\n"
     ]
    }
   ],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "auroc = metrics.roc_auc_score(y_test,prob)\n",
    "auprc = metrics.average_precision_score(y_test,prob)\n",
    "print(\"AUROC\", auroc)\n",
    "print(\"AUPRC\", auprc)\n",
    "\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'neural network ranking',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 0.84 sec\n",
      "Memory usage: 3.10 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.9004412986565081\n",
      "Logit AUPRC 0.2655952893593598\n",
      "XGBoost AUROC 0.8746217999968646\n",
      "XGBoost AUPRC 0.20499697201646291\n",
      "rf AUROC 0.8697502704228002\n",
      "rf AUPRC 0.21197584325983626\n",
      "svm AUROC 0.6670233112292088\n",
      "svm AUPRC 0.2636707578465237\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"Logit AUROC\", auroc)\n",
    "print(\"Logit AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'logistic regression',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"XGBoost AUROC\", auroc)\n",
    "print(\"XGBoost AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'XGBoost',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"rf AUROC\", auroc)\n",
    "print(\"rf AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'random forest',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"svm AUROC\", auroc)\n",
    "print(\"svm AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'support vector machines',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 3.84 sec\n",
      "Memory usage: -1.46 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. embedding dimension: 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 64\n",
    "z_np = pickle.load(open(data_path+f'hybrid_embedding_{embed_dim}_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15444, 64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.8822551369667053\n",
      "test loss 0.706145703792572\n",
      "training loss 0.8575397729873657\n",
      "test loss 0.7001444697380066\n",
      "training loss 0.7320197224617004\n",
      "test loss 0.6952470541000366\n",
      "training loss 0.7483329176902771\n",
      "test loss 0.6929148435592651\n",
      "training loss 0.8247588276863098\n",
      "test loss 0.6889188289642334\n",
      "training loss 0.7423900961875916\n",
      "test loss 0.6838169693946838\n",
      "training loss 0.6707960963249207\n",
      "test loss 0.6793045401573181\n",
      "training loss 0.7847961187362671\n",
      "test loss 0.6754078269004822\n",
      "training loss 0.7033270001411438\n",
      "test loss 0.6717008948326111\n",
      "training loss 0.7160780429840088\n",
      "test loss 0.6680765748023987\n",
      "training loss 0.7272428870201111\n",
      "test loss 0.6631828546524048\n",
      "training loss 0.67169189453125\n",
      "test loss 0.661223292350769\n",
      "training loss 0.6506121754646301\n",
      "test loss 0.660032331943512\n",
      "training loss 0.6574820280075073\n",
      "test loss 0.6522682309150696\n",
      "training loss 0.5831413269042969\n",
      "test loss 0.649164080619812\n",
      "training loss 0.5174868106842041\n",
      "test loss 0.6433387398719788\n",
      "training loss 0.5670253038406372\n",
      "test loss 0.6406354308128357\n",
      "training loss 0.5273466110229492\n",
      "test loss 0.6372954845428467\n",
      "training loss 0.5792466402053833\n",
      "test loss 0.6352961659431458\n",
      "training loss 0.4887302815914154\n",
      "test loss 0.6327564120292664\n",
      "training loss 0.5426768064498901\n",
      "test loss 0.6215839982032776\n",
      "training loss 0.4533960521221161\n",
      "test loss 0.6189922094345093\n",
      "training loss 0.5376905798912048\n",
      "test loss 0.6093243360519409\n",
      "training loss 0.5485637784004211\n",
      "test loss 0.6065223813056946\n",
      "training loss 0.46397995948791504\n",
      "test loss 0.6044961810112\n",
      "training loss 0.5197322964668274\n",
      "test loss 0.6052110195159912\n",
      "training loss 0.47726869583129883\n",
      "test loss 0.5981388092041016\n",
      "training loss 0.5167412161827087\n",
      "test loss 0.5911901593208313\n",
      "training loss 0.499755859375\n",
      "test loss 0.5956803560256958\n",
      "training loss 0.45905083417892456\n",
      "test loss 0.5886337161064148\n"
     ]
    }
   ],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.8840121337534685\n",
      "AUPRC 0.1732987509968906\n"
     ]
    }
   ],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "auroc = metrics.roc_auc_score(y_test,prob)\n",
    "auprc = metrics.average_precision_score(y_test,prob)\n",
    "print(\"AUROC\", auroc)\n",
    "print(\"AUPRC\", auprc)\n",
    "\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'neural network ranking',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 0.62 sec\n",
      "Memory usage: -1.12 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.8848194829829595\n",
      "Logit AUPRC 0.2584255158135413\n",
      "XGBoost AUROC 0.86647776262365\n",
      "XGBoost AUPRC 0.2136404370421257\n",
      "rf AUROC 0.8607087428867046\n",
      "rf AUPRC 0.18629615451725506\n",
      "svm AUROC 0.7374821677718729\n",
      "svm AUPRC 0.22620910728141796\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"Logit AUROC\", auroc)\n",
    "print(\"Logit AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'logistic regression',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"XGBoost AUROC\", auroc)\n",
    "print(\"XGBoost AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'XGBoost',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"rf AUROC\", auroc)\n",
    "print(\"rf AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'random forest',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"svm AUROC\", auroc)\n",
    "print(\"svm AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'support vector machines',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 2.41 sec\n",
      "Memory usage: -0.53 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. embedding dimension: 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15444, 256)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "z_np = pickle.load(open(data_path+f'hybrid_embedding_{embed_dim}_'+exp_id+'.pkl','rb'))\n",
    "z_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 1.1971021890640259\n",
      "test loss 0.6194038391113281\n",
      "training loss 0.6007117629051208\n",
      "test loss 0.5793237686157227\n",
      "training loss 0.4675365686416626\n",
      "test loss 0.6229389309883118\n",
      "training loss 0.37748339772224426\n",
      "test loss 0.6490309238433838\n",
      "training loss 0.43516141176223755\n",
      "test loss 0.6979953050613403\n",
      "training loss 0.5512062907218933\n",
      "test loss 0.6725337505340576\n",
      "training loss 0.48158687353134155\n",
      "test loss 0.7063509225845337\n",
      "training loss 0.5235801339149475\n",
      "test loss 0.7712727189064026\n",
      "training loss 0.4072955250740051\n",
      "test loss 0.7376261353492737\n",
      "training loss 0.6094513535499573\n",
      "test loss 0.8028955459594727\n",
      "training loss 0.5353372097015381\n",
      "test loss 0.789703905582428\n",
      "training loss 0.45833268761634827\n",
      "test loss 0.8465150594711304\n",
      "training loss 0.4590623378753662\n",
      "test loss 0.8092597126960754\n",
      "training loss 0.5528014898300171\n",
      "test loss 0.8178311586380005\n",
      "training loss 0.49342602491378784\n",
      "test loss 0.8162261247634888\n",
      "training loss 0.49464279413223267\n",
      "test loss 0.8329033255577087\n",
      "training loss 0.5122417211532593\n",
      "test loss 0.8370069265365601\n",
      "training loss 0.35543376207351685\n",
      "test loss 0.810664176940918\n",
      "training loss 0.3515021502971649\n",
      "test loss 0.8235426545143127\n",
      "training loss 0.4045022428035736\n",
      "test loss 0.8451719880104065\n",
      "training loss 0.4185555875301361\n",
      "test loss 0.873550534248352\n",
      "training loss 0.4069444239139557\n",
      "test loss 0.8266274333000183\n",
      "training loss 0.43663132190704346\n",
      "test loss 0.79632169008255\n",
      "training loss 0.3867085874080658\n",
      "test loss 0.8143012523651123\n",
      "training loss 0.44188860058784485\n",
      "test loss 0.8038801550865173\n",
      "training loss 0.4383142292499542\n",
      "test loss 0.7767690420150757\n",
      "training loss 0.4056597650051117\n",
      "test loss 0.7239773273468018\n",
      "training loss 0.3904390335083008\n",
      "test loss 0.7382850050926208\n",
      "training loss 0.4064122438430786\n",
      "test loss 0.6977298259735107\n",
      "training loss 0.38083553314208984\n",
      "test loss 0.7403375506401062\n"
     ]
    }
   ],
   "source": [
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)\n",
    "\n",
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.8934338208782079\n",
      "AUPRC 0.24169934575766414\n"
     ]
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt').state_dict())\n",
    "\n",
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "auroc = metrics.roc_auc_score(y_test,prob)\n",
    "auprc = metrics.average_precision_score(y_test,prob)\n",
    "print(\"AUROC\", auroc)\n",
    "print(\"AUPRC\", auprc)\n",
    "\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'neural network ranking',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 1.44 sec\n",
      "Memory usage: 2.74 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.9112111806110771\n",
      "Logit AUPRC 0.3330538576819279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/puzhouwang/opt/anaconda3/envs/drg/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost AUROC 0.8722781357287308\n",
      "XGBoost AUPRC 0.1619359600285969\n",
      "rf AUROC 0.8797206414899121\n",
      "rf AUPRC 0.22246521356421856\n",
      "svm AUROC 0.6692572387088682\n",
      "svm AUPRC 0.25591625669053947\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"Logit AUROC\", auroc)\n",
    "print(\"Logit AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'logistic regression',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"XGBoost AUROC\", auroc)\n",
    "print(\"XGBoost AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'XGBoost',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"rf AUROC\", auroc)\n",
    "print(\"rf AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'random forest',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"svm AUROC\", auroc)\n",
    "print(\"svm AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'support vector machines',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 7.18 sec\n",
      "Memory usage: 6.47 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. without bait-prey edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15444, 128)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "z_np = pickle.load(open(data_path+f'hybrid_embedding_{embed_dim}_no_bp_'+exp_id+'.pkl','rb'))\n",
    "z_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)\n",
    "\n",
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.91648268699646\n",
      "test loss 0.7551467418670654\n",
      "training loss 0.7774485349655151\n",
      "test loss 0.728404700756073\n",
      "training loss 0.7347243428230286\n",
      "test loss 0.7092740535736084\n",
      "training loss 0.7062397003173828\n",
      "test loss 0.6878621578216553\n",
      "training loss 0.6790356040000916\n",
      "test loss 0.6643539071083069\n",
      "training loss 0.6182913184165955\n",
      "test loss 0.6468978524208069\n",
      "training loss 0.5792075991630554\n",
      "test loss 0.631205677986145\n",
      "training loss 0.5830502510070801\n",
      "test loss 0.6173486709594727\n",
      "training loss 0.5432453155517578\n",
      "test loss 0.6000494360923767\n",
      "training loss 0.458672434091568\n",
      "test loss 0.5892173051834106\n",
      "training loss 0.5596255660057068\n",
      "test loss 0.5636516809463501\n",
      "training loss 0.45072367787361145\n",
      "test loss 0.5616510510444641\n",
      "training loss 0.5626056790351868\n",
      "test loss 0.5386508703231812\n",
      "training loss 0.5114240646362305\n",
      "test loss 0.536547064781189\n",
      "training loss 0.45689332485198975\n",
      "test loss 0.5256733298301697\n",
      "training loss 0.47503483295440674\n",
      "test loss 0.5071656107902527\n",
      "training loss 0.4326924979686737\n",
      "test loss 0.5110179781913757\n",
      "training loss 0.4844890534877777\n",
      "test loss 0.47850173711776733\n",
      "training loss 0.427166223526001\n",
      "test loss 0.4726116955280304\n",
      "training loss 0.3817659914493561\n",
      "test loss 0.48095735907554626\n",
      "training loss 0.36184707283973694\n",
      "test loss 0.4674934446811676\n",
      "training loss 0.44658440351486206\n",
      "test loss 0.463901549577713\n",
      "training loss 0.5212912559509277\n",
      "test loss 0.46697989106178284\n",
      "training loss 0.38483336567878723\n",
      "test loss 0.4748257100582123\n",
      "training loss 0.44485750794410706\n",
      "test loss 0.44757434725761414\n",
      "training loss 0.44358131289482117\n",
      "test loss 0.4632076919078827\n",
      "training loss 0.4144316613674164\n",
      "test loss 0.45774969458580017\n",
      "training loss 0.34158509969711304\n",
      "test loss 0.43356388807296753\n",
      "training loss 0.4157918393611908\n",
      "test loss 0.4485252797603607\n",
      "training loss 0.3350372612476349\n",
      "test loss 0.45026713609695435\n"
     ]
    }
   ],
   "source": [
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)\n",
    "\n",
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+f'nn_clf_hybrid_embedding_{embed_dim}_no_bp_.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.7666721535060904\n",
      "AUPRC 0.08683337002903749\n"
     ]
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt').state_dict())\n",
    "\n",
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "auroc = metrics.roc_auc_score(y_test,prob)\n",
    "auprc = metrics.average_precision_score(y_test,prob)\n",
    "print(\"AUROC\", auroc)\n",
    "print(\"AUPRC\", auprc)\n",
    "\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid without bait-prey',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'neural network ranking',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 0.80 sec\n",
      "Memory usage: 1.33 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.8839415886751634\n",
      "Logit AUPRC 0.22722804899906635\n",
      "XGBoost AUROC 0.8784939409616078\n",
      "XGBoost AUPRC 0.17386730653022386\n",
      "rf AUROC 0.8432841085453605\n",
      "rf AUPRC 0.1272555533653837\n",
      "svm AUROC 0.7566625907288091\n",
      "svm AUPRC 0.17287456132624332\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"Logit AUROC\", auroc)\n",
    "print(\"Logit AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid without bait-prey',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'logistic regression',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"XGBoost AUROC\", auroc)\n",
    "print(\"XGBoost AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid without bait-prey',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'XGBoost',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"rf AUROC\", auroc)\n",
    "print(\"rf AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid without bait-prey',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'random forest',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"svm AUROC\", auroc)\n",
    "print(\"svm AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid without bait-prey',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'support vector machines',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 4.23 sec\n",
      "Memory usage: 5.62 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. save all AUROC and AUPRC results for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_sum = pd.DataFrame(list_dict)\n",
    "df_sum.to_csv(f'df_sum_{exp_id}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drg",
   "language": "python",
   "name": "drg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
