{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the AUROC and AUPRC results for plotting\n",
    "list_dict = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to track CPU time and Memory usage\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "start_memory = psutil.virtual_memory().available\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.COVID-19 graph embedding alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, average_precision_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules import Module\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='data/'\n",
    "exp_id='v0'\n",
    "device_id='cpu' #'cpu' if CPU, device number if GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=pickle.load(open(data_path+'LabelEncoder_'+exp_id+'.pkl', 'rb'))\n",
    "edge_index=pickle.load(open(data_path+'edge_index_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types=np.array([item.split('_')[0] for item in le.classes_ ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label\n",
    "trials=pd.read_excel(data_path+'literature-mining/All_trails_5_24.xlsx',header=1,index_col=0)\n",
    "trials_drug=set([drug.strip().upper() for lst in trials.loc[trials['study_category'].apply(lambda x: 'drug' in x.lower()),'intervention'].apply(lambda x: re.split(r'[+|/|,]',x.replace(' vs. ', '/').replace(' vs ', '/').replace(' or ', '/').replace(' with and without ', '/').replace(' /wo ', '/').replace(' /w ', '/').replace(' and ', '/').replace(' - ', '/').replace(' (', '/').replace(') ', '/'))).values for drug in lst])\n",
    "drug_labels=[1 if drug.split('_')[1] in trials_drug else 0 for drug in le.classes_[types=='drug'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self,embedding_dim):\n",
    "        super(Classifier, self).__init__() \n",
    "        self.fc1=nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.fc2=nn.Linear(embedding_dim,1)\n",
    "        self.bn=nn.BatchNorm1d(embedding_dim)\n",
    "    def forward(self, x):\n",
    "        residual1 = x\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x= self.bn(F.dropout(F.relu(self.fc1(x)),training=self.training))\n",
    "        x += residual1  \n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import BatchSampler, WeightedRandomSampler\n",
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self, num_neg_samples):\n",
    "        super(BPRLoss, self).__init__()\n",
    "        self.num_neg_samples=num_neg_samples\n",
    "    \n",
    "    def forward(self, output, label):\n",
    "        positive_output=output[label==1]\n",
    "        negative_output=output[label!=1]\n",
    "        \n",
    "        #negative sample proportional to the high values\n",
    "        negative_sampler=WeightedRandomSampler(negative_output-min(negative_output), num_samples=self.num_neg_samples*len(positive_output),replacement=True)\n",
    "        negative_sample_output=negative_output[torch.tensor(list(BatchSampler(negative_sampler, batch_size=len(positive_output),drop_last=True)),dtype=torch.long).t()]\n",
    "        return -(positive_output.view(-1,1)-negative_sample_output).sigmoid().log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_np = pickle.load(open(data_path+'COVID_embedding_'+exp_id+'.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+'nn_clf_covid_embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.load_state_dict(torch.load(data_path+'nn_clf_covid_embedding.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "auroc = metrics.roc_auc_score(y_test,prob)\n",
    "auprc = metrics.average_precision_score(y_test,prob)\n",
    "print(\"AUROC\", auroc)\n",
    "print(\"AUPRC\", auprc)\n",
    "\n",
    "list_dict.append({\n",
    "    'embed_method': 'COVID-19 alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'neural network ranking',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"Logit AUROC\", auroc)\n",
    "print(\"Logit AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'COVID-19 alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'logistic regression',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"XGBoost AUROC\", auroc)\n",
    "print(\"XGBoost AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'COVID-19 alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'XGBoost',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"rf AUROC\", auroc)\n",
    "print(\"rf AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'COVID-19 alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'random forest',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"svm AUROC\", auroc)\n",
    "print(\"svm AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'COVID-19 alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'support vector machines',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DRKG embedding alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_np = pickle.load(open(data_path+'node_feature_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 400\n",
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+'nn_clf_DRKG_embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.load_state_dict(torch.load(data_path+'nn_clf_DRKG_embedding.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "auroc = metrics.roc_auc_score(y_test,prob)\n",
    "auprc = metrics.average_precision_score(y_test,prob)\n",
    "print(\"AUROC\", auroc)\n",
    "print(\"AUPRC\", auprc)\n",
    "\n",
    "list_dict.append({\n",
    "    'embed_method': 'DRKG alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'neural network ranking',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"Logit AUROC\", auroc)\n",
    "print(\"Logit AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'DRKG alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'logistic regression',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"XGBoost AUROC\", auroc)\n",
    "print(\"XGBoost AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'DRKG alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'XGBoost',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"rf AUROC\", auroc)\n",
    "print(\"rf AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'DRKG alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'random forest',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"svm AUROC\", auroc)\n",
    "print(\"svm AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'DRKG alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'support vector machines',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. hybrid embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. embedding dimension: 128 (default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "z_np = pickle.load(open(data_path+f'hybrid_embedding_{embed_dim}_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.load_state_dict(torch.load(data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "auroc = metrics.roc_auc_score(y_test,prob)\n",
    "auprc = metrics.average_precision_score(y_test,prob)\n",
    "print(\"AUROC\", auroc)\n",
    "print(\"AUPRC\", auprc)\n",
    "\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'neural network ranking',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"Logit AUROC\", auroc)\n",
    "print(\"Logit AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'logistic regression',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"XGBoost AUROC\", auroc)\n",
    "print(\"XGBoost AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'XGBoost',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"rf AUROC\", auroc)\n",
    "print(\"rf AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'random forest',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"svm AUROC\", auroc)\n",
    "print(\"svm AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'support vector machines',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. embedding dimension: 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 64\n",
    "z_np = pickle.load(open(data_path+f'hybrid_embedding_{embed_dim}_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.load_state_dict(torch.load(data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "auroc = metrics.roc_auc_score(y_test,prob)\n",
    "auprc = metrics.average_precision_score(y_test,prob)\n",
    "print(\"AUROC\", auroc)\n",
    "print(\"AUPRC\", auprc)\n",
    "\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'neural network ranking',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"Logit AUROC\", auroc)\n",
    "print(\"Logit AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'logistic regression',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"XGBoost AUROC\", auroc)\n",
    "print(\"XGBoost AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'XGBoost',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"rf AUROC\", auroc)\n",
    "print(\"rf AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'random forest',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"svm AUROC\", auroc)\n",
    "print(\"svm AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'support vector machines',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. embedding dimension: 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "z_np = pickle.load(open(data_path+f'hybrid_embedding_{embed_dim}_'+exp_id+'.pkl','rb'))\n",
    "z_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)\n",
    "\n",
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.load_state_dict(torch.load(data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt').state_dict())\n",
    "\n",
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "auroc = metrics.roc_auc_score(y_test,prob)\n",
    "auprc = metrics.average_precision_score(y_test,prob)\n",
    "print(\"AUROC\", auroc)\n",
    "print(\"AUPRC\", auprc)\n",
    "\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'neural network ranking',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"Logit AUROC\", auroc)\n",
    "print(\"Logit AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'logistic regression',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"XGBoost AUROC\", auroc)\n",
    "print(\"XGBoost AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'XGBoost',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"rf AUROC\", auroc)\n",
    "print(\"rf AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'random forest',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"svm AUROC\", auroc)\n",
    "print(\"svm AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'support vector machines',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. without bait-prey edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "z_np = pickle.load(open(data_path+f'hybrid_embedding_{embed_dim}_no_bp_'+exp_id+'.pkl','rb'))\n",
    "z_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)\n",
    "\n",
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)\n",
    "\n",
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+f'nn_clf_hybrid_embedding_{embed_dim}_no_bp_.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.load_state_dict(torch.load(data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt').state_dict())\n",
    "\n",
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "auroc = metrics.roc_auc_score(y_test,prob)\n",
    "auprc = metrics.average_precision_score(y_test,prob)\n",
    "print(\"AUROC\", auroc)\n",
    "print(\"AUPRC\", auprc)\n",
    "\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid without bait-prey',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'neural network ranking',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"Logit AUROC\", auroc)\n",
    "print(\"Logit AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid without bait-prey',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'logistic regression',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"XGBoost AUROC\", auroc)\n",
    "print(\"XGBoost AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid without bait-prey',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'XGBoost',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"rf AUROC\", auroc)\n",
    "print(\"rf AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid without bait-prey',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'random forest',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"svm AUROC\", auroc)\n",
    "print(\"svm AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid without bait-prey',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'support vector machines',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. save all AUROC and AUPRC results for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_sum = pd.DataFrame(list_dict)\n",
    "df_sum.to_csv(f'df_sum_{exp_id}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drg",
   "language": "python",
   "name": "drg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
