{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.COVID-19 graph embedding alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, average_precision_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules import Module\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='data/'\n",
    "exp_id='v0'\n",
    "device_id='cpu' #'cpu' if CPU, device number if GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=pickle.load(open(data_path+'LabelEncoder_'+exp_id+'.pkl', 'rb'))\n",
    "edge_index=pickle.load(open(data_path+'edge_index_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "types=np.array([item.split('_')[0] for item in le.classes_ ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label\n",
    "trials=pd.read_excel(data_path+'literature-mining/All_trails_5_24.xlsx',header=1,index_col=0)\n",
    "trials_drug=set([drug.strip().upper() for lst in trials.loc[trials['study_category'].apply(lambda x: 'drug' in x.lower()),'intervention'].apply(lambda x: re.split(r'[+|/|,]',x.replace(' vs. ', '/').replace(' vs ', '/').replace(' or ', '/').replace(' with and without ', '/').replace(' /wo ', '/').replace(' /w ', '/').replace(' and ', '/').replace(' - ', '/').replace(' (', '/').replace(') ', '/'))).values for drug in lst])\n",
    "drug_labels=[1 if drug.split('_')[1] in trials_drug else 0 for drug in le.classes_[types=='drug'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_np = pickle.load(open(data_path+'COVID_embedding_'+exp_id+'.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self,embedding_dim):\n",
    "        super(Classifier, self).__init__() \n",
    "        self.fc1=nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.fc2=nn.Linear(embedding_dim,1)\n",
    "        self.bn=nn.BatchNorm1d(embedding_dim)\n",
    "    def forward(self, x):\n",
    "        residual1 = x\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x= self.bn(F.dropout(F.relu(self.fc1(x)),training=self.training))\n",
    "        x += residual1  \n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import BatchSampler, WeightedRandomSampler\n",
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self, num_neg_samples):\n",
    "        super(BPRLoss, self).__init__()\n",
    "        self.num_neg_samples=num_neg_samples\n",
    "    \n",
    "    def forward(self, output, label):\n",
    "        positive_output=output[label==1]\n",
    "        negative_output=output[label!=1]\n",
    "        \n",
    "        #negative sample proportional to the high values\n",
    "        negative_sampler=WeightedRandomSampler(negative_output-min(negative_output), num_samples=self.num_neg_samples*len(positive_output),replacement=True)\n",
    "        negative_sample_output=negative_output[torch.tensor(list(BatchSampler(negative_sampler, batch_size=len(positive_output),drop_last=True)),dtype=torch.long).t()]\n",
    "        return -(positive_output.view(-1,1)-negative_sample_output).sigmoid().log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Classifier(embedding_dim=128).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.7957777380943298\n",
      "test loss 0.6983648538589478\n",
      "training loss 0.8004921078681946\n",
      "test loss 0.6988610625267029\n",
      "training loss 0.7474350333213806\n",
      "test loss 0.6995164752006531\n",
      "training loss 0.8172909617424011\n",
      "test loss 0.7001990079879761\n",
      "training loss 0.840722382068634\n",
      "test loss 0.7008649706840515\n",
      "training loss 0.7738116979598999\n",
      "test loss 0.7015867829322815\n",
      "training loss 0.8056227564811707\n",
      "test loss 0.7023756504058838\n",
      "training loss 0.8166890740394592\n",
      "test loss 0.7031198740005493\n",
      "training loss 0.7883610725402832\n",
      "test loss 0.7038981914520264\n",
      "training loss 0.8199113011360168\n",
      "test loss 0.7047076225280762\n",
      "training loss 0.8332513570785522\n",
      "test loss 0.7057130336761475\n",
      "training loss 0.8371492624282837\n",
      "test loss 0.7067953944206238\n",
      "training loss 0.8037989139556885\n",
      "test loss 0.7077513337135315\n",
      "training loss 0.763800859451294\n",
      "test loss 0.7088028788566589\n",
      "training loss 0.7703692317008972\n",
      "test loss 0.7099213600158691\n",
      "training loss 0.8040730953216553\n",
      "test loss 0.7110167145729065\n",
      "training loss 0.8660479187965393\n",
      "test loss 0.7121759653091431\n",
      "training loss 0.7272461652755737\n",
      "test loss 0.7132896184921265\n",
      "training loss 0.7362297177314758\n",
      "test loss 0.7145248651504517\n",
      "training loss 0.8000311851501465\n",
      "test loss 0.7159066200256348\n",
      "training loss 0.7529032230377197\n",
      "test loss 0.717496395111084\n",
      "training loss 0.744015097618103\n",
      "test loss 0.7192011475563049\n",
      "training loss 0.7825211882591248\n",
      "test loss 0.7209895849227905\n",
      "training loss 0.7532190084457397\n",
      "test loss 0.7228346467018127\n",
      "training loss 0.8369739651679993\n",
      "test loss 0.7246559262275696\n",
      "training loss 0.8180505037307739\n",
      "test loss 0.7266517281532288\n",
      "training loss 0.7908828854560852\n",
      "test loss 0.7286916971206665\n",
      "training loss 0.7708283066749573\n",
      "test loss 0.7310211658477783\n",
      "training loss 0.786538302898407\n",
      "test loss 0.7333587408065796\n",
      "training loss 0.7962713241577148\n",
      "test loss 0.7358803749084473\n"
     ]
    }
   ],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+'nn_clf_covid_embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+'nn_clf_covid_embedding.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.5068115192274529\n",
      "AUPRC 0.04367524330005571\n"
     ]
    }
   ],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "print(\"AUROC\", metrics.roc_auc_score(y_test,prob))\n",
    "print(\"AUPRC\", metrics.average_precision_score(y_test,prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.4619526877674833\n",
      "Logit AUPRC 0.09362296208531114\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "print(\"Logit AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"Logit AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost AUROC 0.4992475191647463\n",
      "XGBoost AUPRC 0.04322014411352604\n"
     ]
    }
   ],
   "source": [
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "print(\"XGBoost AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"XGBoost AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf AUROC 0.4992475191647463\n",
      "rf AUPRC 0.04322014411352604\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "print(\"rf AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"rf AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm AUROC 0.36704604242110705\n",
      "svm AUPRC 0.07614197571740616\n"
     ]
    }
   ],
   "source": [
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "print(\"svm AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"svm AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DRKG embedding alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_np = pickle.load(open(data_path+'node_feature_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Classifier(embedding_dim=400).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.7395689487457275\n",
      "test loss 0.616218090057373\n",
      "training loss 0.4780997633934021\n",
      "test loss 0.5486316084861755\n",
      "training loss 0.3808656632900238\n",
      "test loss 0.5048272013664246\n",
      "training loss 0.27103176712989807\n",
      "test loss 0.4665544927120209\n",
      "training loss 0.309094101190567\n",
      "test loss 0.4589807391166687\n",
      "training loss 0.2699897885322571\n",
      "test loss 0.460541695356369\n",
      "training loss 0.24963025748729706\n",
      "test loss 0.4448689818382263\n",
      "training loss 0.3198316991329193\n",
      "test loss 0.4421408176422119\n",
      "training loss 0.2551022469997406\n",
      "test loss 0.43925824761390686\n",
      "training loss 0.3009866774082184\n",
      "test loss 0.4519835114479065\n",
      "training loss 0.19218678772449493\n",
      "test loss 0.4679725766181946\n",
      "training loss 0.24697791039943695\n",
      "test loss 0.4696771800518036\n",
      "training loss 0.26026809215545654\n",
      "test loss 0.44761529564857483\n",
      "training loss 0.1606442779302597\n",
      "test loss 0.4597027897834778\n",
      "training loss 0.19212880730628967\n",
      "test loss 0.4595140814781189\n",
      "training loss 0.2429376095533371\n",
      "test loss 0.47952720522880554\n",
      "training loss 0.2554340362548828\n",
      "test loss 0.47675156593322754\n",
      "training loss 0.17326855659484863\n",
      "test loss 0.44742801785469055\n",
      "training loss 0.22081337869167328\n",
      "test loss 0.4494136869907379\n",
      "training loss 0.15048331022262573\n",
      "test loss 0.45945194363594055\n",
      "training loss 0.2371065467596054\n",
      "test loss 0.4462662637233734\n",
      "training loss 0.25639843940734863\n",
      "test loss 0.4469963312149048\n",
      "training loss 0.19932150840759277\n",
      "test loss 0.4287112057209015\n",
      "training loss 0.16300223767757416\n",
      "test loss 0.442425936460495\n",
      "training loss 0.15473993122577667\n",
      "test loss 0.4268935024738312\n",
      "training loss 0.19608443975448608\n",
      "test loss 0.44180670380592346\n",
      "training loss 0.16495317220687866\n",
      "test loss 0.4128867983818054\n",
      "training loss 0.18801726400852203\n",
      "test loss 0.42488813400268555\n",
      "training loss 0.12063349783420563\n",
      "test loss 0.4169601500034332\n",
      "training loss 0.14785699546337128\n",
      "test loss 0.40083518624305725\n"
     ]
    }
   ],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+'nn_clf_DRKG_embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+'nn_clf_DRKG_embedding.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.8667481854238193\n",
      "AUPRC 0.18218957502766814\n"
     ]
    }
   ],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "print(\"AUROC\", metrics.roc_auc_score(y_test,prob))\n",
    "print(\"AUPRC\", metrics.average_precision_score(y_test,prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.8436015613977331\n",
      "Logit AUPRC 0.18394788718419244\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "print(\"Logit AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"Logit AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost AUROC 0.8554609728950132\n",
      "XGBoost AUPRC 0.13820130901483108\n"
     ]
    }
   ],
   "source": [
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "print(\"XGBoost AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"XGBoost AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf AUROC 0.855993980153318\n",
      "rf AUPRC 0.13835021652961998\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "print(\"rf AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"rf AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm AUROC 0.8842276881594006\n",
      "svm AUPRC 0.2578582442037748\n"
     ]
    }
   ],
   "source": [
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "print(\"svm AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"svm AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. hybrid embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_np = pickle.load(open(data_path+'hybrid_embedding_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Classifier(embedding_dim=128).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.9536146521568298\n",
      "test loss 0.7626969218254089\n",
      "training loss 0.7878918647766113\n",
      "test loss 0.7334699630737305\n",
      "training loss 0.7278525233268738\n",
      "test loss 0.705784261226654\n",
      "training loss 0.6320992708206177\n",
      "test loss 0.678355872631073\n",
      "training loss 0.5433192253112793\n",
      "test loss 0.6581498980522156\n",
      "training loss 0.48433196544647217\n",
      "test loss 0.6377235054969788\n",
      "training loss 0.5341165661811829\n",
      "test loss 0.6118820309638977\n",
      "training loss 0.49445411562919617\n",
      "test loss 0.6034524440765381\n",
      "training loss 0.48542895913124084\n",
      "test loss 0.5843536853790283\n",
      "training loss 0.4485393464565277\n",
      "test loss 0.5750580430030823\n",
      "training loss 0.49111926555633545\n",
      "test loss 0.557050347328186\n",
      "training loss 0.41423916816711426\n",
      "test loss 0.5427328944206238\n",
      "training loss 0.481771320104599\n",
      "test loss 0.5426845550537109\n",
      "training loss 0.3665356934070587\n",
      "test loss 0.5306793451309204\n",
      "training loss 0.4309069812297821\n",
      "test loss 0.5272321701049805\n",
      "training loss 0.40130066871643066\n",
      "test loss 0.5076251029968262\n",
      "training loss 0.38337481021881104\n",
      "test loss 0.5096538662910461\n",
      "training loss 0.43279168009757996\n",
      "test loss 0.5049124956130981\n",
      "training loss 0.42559075355529785\n",
      "test loss 0.5014035701751709\n",
      "training loss 0.40006864070892334\n",
      "test loss 0.5084820985794067\n",
      "training loss 0.34783387184143066\n",
      "test loss 0.4995178282260895\n",
      "training loss 0.40393638610839844\n",
      "test loss 0.5058469176292419\n",
      "training loss 0.4118081033229828\n",
      "test loss 0.505551278591156\n",
      "training loss 0.35282862186431885\n",
      "test loss 0.4837970733642578\n",
      "training loss 0.34564730525016785\n",
      "test loss 0.4910026490688324\n",
      "training loss 0.3663967251777649\n",
      "test loss 0.4969695210456848\n",
      "training loss 0.3590196967124939\n",
      "test loss 0.4837315082550049\n",
      "training loss 0.39851927757263184\n",
      "test loss 0.49683818221092224\n",
      "training loss 0.3068998157978058\n",
      "test loss 0.481640487909317\n",
      "training loss 0.3348841667175293\n",
      "test loss 0.5002345442771912\n"
     ]
    }
   ],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+'nn_clf_hybrid_embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+'nn_clf_hybrid_embedding.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.8930654188026148\n",
      "AUPRC 0.27073473253095137\n"
     ]
    }
   ],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "print(\"AUROC\", metrics.roc_auc_score(y_test,prob))\n",
    "print(\"AUPRC\", metrics.average_precision_score(y_test,prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.9045877815924376\n",
      "Logit AUPRC 0.3189766638941174\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "print(\"Logit AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"Logit AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost AUROC 0.8656508175390742\n",
      "XGBoost AUPRC 0.20530366246464798\n"
     ]
    }
   ],
   "source": [
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "print(\"XGBoost AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"XGBoost AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf AUROC 0.8670186082239885\n",
      "rf AUPRC 0.252456982482454\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "print(\"rf AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"rf AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm AUROC 0.8549083697816238\n",
      "svm AUPRC 0.26104412389180137\n"
     ]
    }
   ],
   "source": [
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "print(\"svm AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"svm AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drg",
   "language": "python",
   "name": "drg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
