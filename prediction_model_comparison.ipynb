{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block to track CPU time and Memory usage\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "start_memory = psutil.virtual_memory().available\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.COVID-19 graph embedding alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, average_precision_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules import Module\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='data/'\n",
    "exp_id='v0'\n",
    "device_id='cpu' #'cpu' if CPU, device number if GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=pickle.load(open(data_path+'LabelEncoder_'+exp_id+'.pkl', 'rb'))\n",
    "edge_index=pickle.load(open(data_path+'edge_index_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "types=np.array([item.split('_')[0] for item in le.classes_ ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label\n",
    "trials=pd.read_excel(data_path+'literature-mining/All_trails_5_24.xlsx',header=1,index_col=0)\n",
    "trials_drug=set([drug.strip().upper() for lst in trials.loc[trials['study_category'].apply(lambda x: 'drug' in x.lower()),'intervention'].apply(lambda x: re.split(r'[+|/|,]',x.replace(' vs. ', '/').replace(' vs ', '/').replace(' or ', '/').replace(' with and without ', '/').replace(' /wo ', '/').replace(' /w ', '/').replace(' and ', '/').replace(' - ', '/').replace(' (', '/').replace(') ', '/'))).values for drug in lst])\n",
    "drug_labels=[1 if drug.split('_')[1] in trials_drug else 0 for drug in le.classes_[types=='drug'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_np = pickle.load(open(data_path+'COVID_embedding_'+exp_id+'.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self,embedding_dim):\n",
    "        super(Classifier, self).__init__() \n",
    "        self.fc1=nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.fc2=nn.Linear(embedding_dim,1)\n",
    "        self.bn=nn.BatchNorm1d(embedding_dim)\n",
    "    def forward(self, x):\n",
    "        residual1 = x\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x= self.bn(F.dropout(F.relu(self.fc1(x)),training=self.training))\n",
    "        x += residual1  \n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import BatchSampler, WeightedRandomSampler\n",
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self, num_neg_samples):\n",
    "        super(BPRLoss, self).__init__()\n",
    "        self.num_neg_samples=num_neg_samples\n",
    "    \n",
    "    def forward(self, output, label):\n",
    "        positive_output=output[label==1]\n",
    "        negative_output=output[label!=1]\n",
    "        \n",
    "        #negative sample proportional to the high values\n",
    "        negative_sampler=WeightedRandomSampler(negative_output-min(negative_output), num_samples=self.num_neg_samples*len(positive_output),replacement=True)\n",
    "        negative_sample_output=negative_output[torch.tensor(list(BatchSampler(negative_sampler, batch_size=len(positive_output),drop_last=True)),dtype=torch.long).t()]\n",
    "        return -(positive_output.view(-1,1)-negative_sample_output).sigmoid().log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Classifier(embedding_dim=128).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.7809965014457703\n",
      "test loss 0.6931513547897339\n",
      "training loss 0.804292619228363\n",
      "test loss 0.693146824836731\n",
      "training loss 0.7770630121231079\n",
      "test loss 0.693117082118988\n",
      "training loss 0.7913481593132019\n",
      "test loss 0.6930703520774841\n",
      "training loss 0.7717733383178711\n",
      "test loss 0.6930720806121826\n",
      "training loss 0.7628346681594849\n",
      "test loss 0.6930191516876221\n",
      "training loss 0.7237057089805603\n",
      "test loss 0.6929858922958374\n",
      "training loss 0.8377503156661987\n",
      "test loss 0.6929678916931152\n",
      "training loss 0.8397920727729797\n",
      "test loss 0.6929134130477905\n",
      "training loss 0.778126060962677\n",
      "test loss 0.6929548382759094\n",
      "training loss 0.7872323393821716\n",
      "test loss 0.6928908824920654\n",
      "training loss 0.7686043977737427\n",
      "test loss 0.6928220391273499\n",
      "training loss 0.8155762553215027\n",
      "test loss 0.6928841471672058\n",
      "training loss 0.8421730995178223\n",
      "test loss 0.6927823424339294\n",
      "training loss 0.8292374610900879\n",
      "test loss 0.692762553691864\n",
      "training loss 0.8756214380264282\n",
      "test loss 0.6927294731140137\n",
      "training loss 0.7791784405708313\n",
      "test loss 0.6928460597991943\n",
      "training loss 0.7831748127937317\n",
      "test loss 0.6925619840621948\n",
      "training loss 0.7873750925064087\n",
      "test loss 0.6927419900894165\n",
      "training loss 0.716541051864624\n",
      "test loss 0.6925308704376221\n",
      "training loss 0.7833334803581238\n",
      "test loss 0.6926091313362122\n",
      "training loss 0.7656248211860657\n",
      "test loss 0.6928138136863708\n",
      "training loss 0.7906980514526367\n",
      "test loss 0.6926777362823486\n",
      "training loss 0.7594365477561951\n",
      "test loss 0.6928316950798035\n",
      "training loss 0.8092932105064392\n",
      "test loss 0.6928441524505615\n",
      "training loss 0.7080410122871399\n",
      "test loss 0.6927456855773926\n",
      "training loss 0.8040132522583008\n",
      "test loss 0.6928418874740601\n",
      "training loss 0.7785192728042603\n",
      "test loss 0.6928963661193848\n",
      "training loss 0.8204330205917358\n",
      "test loss 0.6926254034042358\n",
      "training loss 0.7729595899581909\n",
      "test loss 0.6924015283584595\n"
     ]
    }
   ],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+'nn_clf_covid_embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+'nn_clf_covid_embedding.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.660125570239383\n",
      "AUPRC 0.07690719158863707\n"
     ]
    }
   ],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "print(\"AUROC\", metrics.roc_auc_score(y_test,prob))\n",
    "print(\"AUPRC\", metrics.average_precision_score(y_test,prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 3.73 sec\n",
      "Memory usage: 31.00 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) >> 20\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.660858455219552\n",
      "Logit AUPRC 0.10996734653884108\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "print(\"Logit AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"Logit AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost AUROC 0.6202088134317829\n",
      "XGBoost AUPRC 0.05263595141059285\n"
     ]
    }
   ],
   "source": [
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "print(\"XGBoost AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"XGBoost AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf AUROC 0.6187979118656822\n",
      "rf AUPRC 0.06268249294663361\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "print(\"rf AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"rf AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm AUROC 0.6230776466161877\n",
      "svm AUPRC 0.09671634818156807\n"
     ]
    }
   ],
   "source": [
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "print(\"svm AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"svm AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 7.22 sec\n",
      "Memory usage: -2.00 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) >> 20\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DRKG embedding alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_np = pickle.load(open(data_path+'node_feature_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Classifier(embedding_dim=400).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 1.095960259437561\n",
      "test loss 0.5509073734283447\n",
      "training loss 0.5284356474876404\n",
      "test loss 0.48954105377197266\n",
      "training loss 0.3158768117427826\n",
      "test loss 0.4442881643772125\n",
      "training loss 0.3188949227333069\n",
      "test loss 0.4358430504798889\n",
      "training loss 0.2867802381515503\n",
      "test loss 0.4540466368198395\n",
      "training loss 0.2857513427734375\n",
      "test loss 0.45952141284942627\n",
      "training loss 0.30420947074890137\n",
      "test loss 0.4615301787853241\n",
      "training loss 0.23983801901340485\n",
      "test loss 0.46274715662002563\n",
      "training loss 0.2406436949968338\n",
      "test loss 0.4637373685836792\n",
      "training loss 0.2719497084617615\n",
      "test loss 0.47666019201278687\n",
      "training loss 0.2533254027366638\n",
      "test loss 0.5014693737030029\n",
      "training loss 0.2949785888195038\n",
      "test loss 0.5112943649291992\n",
      "training loss 0.23634734749794006\n",
      "test loss 0.5076175332069397\n",
      "training loss 0.22079740464687347\n",
      "test loss 0.5060307383537292\n",
      "training loss 0.20401377975940704\n",
      "test loss 0.5226432681083679\n",
      "training loss 0.22306714951992035\n",
      "test loss 0.529446542263031\n",
      "training loss 0.2634526491165161\n",
      "test loss 0.5042685866355896\n",
      "training loss 0.23490120470523834\n",
      "test loss 0.4847710430622101\n",
      "training loss 0.22702987492084503\n",
      "test loss 0.5576868653297424\n",
      "training loss 0.2406783103942871\n",
      "test loss 0.49705594778060913\n",
      "training loss 0.2063656598329544\n",
      "test loss 0.5159837603569031\n",
      "training loss 0.2019621878862381\n",
      "test loss 0.5194514393806458\n",
      "training loss 0.18226221203804016\n",
      "test loss 0.49609875679016113\n",
      "training loss 0.19861505925655365\n",
      "test loss 0.48792561888694763\n",
      "training loss 0.1569400280714035\n",
      "test loss 0.4947052299976349\n",
      "training loss 0.15838752686977386\n",
      "test loss 0.4815523326396942\n",
      "training loss 0.17043232917785645\n",
      "test loss 0.5042126178741455\n",
      "training loss 0.16304370760917664\n",
      "test loss 0.48297733068466187\n",
      "training loss 0.174960196018219\n",
      "test loss 0.4580036699771881\n",
      "training loss 0.1450888067483902\n",
      "test loss 0.5290060043334961\n"
     ]
    }
   ],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+'nn_clf_DRKG_embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+'nn_clf_DRKG_embedding.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.8673203843922933\n",
      "AUPRC 0.16921407124724994\n"
     ]
    }
   ],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "print(\"AUROC\", metrics.roc_auc_score(y_test,prob))\n",
    "print(\"AUPRC\", metrics.average_precision_score(y_test,prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 2.64 sec\n",
      "Memory usage: 17.00 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) >> 20\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.8436015613977331\n",
      "Logit AUPRC 0.18394788718419244\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "print(\"Logit AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"Logit AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost AUROC 0.8452084215146812\n",
      "XGBoost AUPRC 0.12854067205592395\n"
     ]
    }
   ],
   "source": [
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "print(\"XGBoost AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"XGBoost AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf AUROC 0.8488610889024752\n",
      "rf AUPRC 0.127652906753775\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "print(\"rf AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"rf AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm AUROC 0.8842276881594006\n",
      "svm AUPRC 0.2578582442037748\n"
     ]
    }
   ],
   "source": [
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "print(\"svm AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"svm AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 5.94 sec\n",
      "Memory usage: -2.00 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) >> 20\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. hybrid embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. embedding dimension: 128 (default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "z_np = pickle.load(open(data_path+f'hybrid_embedding_{embed_dim}_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 1.017173171043396\n",
      "test loss 0.638919472694397\n",
      "training loss 0.8258885741233826\n",
      "test loss 0.6025058031082153\n",
      "training loss 0.6619513630867004\n",
      "test loss 0.5783891081809998\n",
      "training loss 0.47295838594436646\n",
      "test loss 0.5564704537391663\n",
      "training loss 0.4943515658378601\n",
      "test loss 0.5463363528251648\n",
      "training loss 0.4983232319355011\n",
      "test loss 0.5563392043113708\n",
      "training loss 0.3930683135986328\n",
      "test loss 0.5492562055587769\n",
      "training loss 0.4059927463531494\n",
      "test loss 0.5570959448814392\n",
      "training loss 0.44118326902389526\n",
      "test loss 0.5508155226707458\n",
      "training loss 0.4223344922065735\n",
      "test loss 0.5619780421257019\n",
      "training loss 0.4663914144039154\n",
      "test loss 0.5443903207778931\n",
      "training loss 0.42017337679862976\n",
      "test loss 0.5878337025642395\n",
      "training loss 0.4590558111667633\n",
      "test loss 0.5901212096214294\n",
      "training loss 0.4580325782299042\n",
      "test loss 0.5968707799911499\n",
      "training loss 0.427585631608963\n",
      "test loss 0.6201627254486084\n",
      "training loss 0.4755777418613434\n",
      "test loss 0.6111398339271545\n",
      "training loss 0.45293915271759033\n",
      "test loss 0.6474121809005737\n",
      "training loss 0.5878097414970398\n",
      "test loss 0.6257780194282532\n",
      "training loss 0.4224853217601776\n",
      "test loss 0.6584581732749939\n",
      "training loss 0.3488716185092926\n",
      "test loss 0.6256564855575562\n",
      "training loss 0.47331351041793823\n",
      "test loss 0.6280471682548523\n",
      "training loss 0.3833008110523224\n",
      "test loss 0.6816114187240601\n",
      "training loss 0.4005129933357239\n",
      "test loss 0.6706715822219849\n",
      "training loss 0.3852197527885437\n",
      "test loss 0.6689809560775757\n",
      "training loss 0.3979876935482025\n",
      "test loss 0.7024683356285095\n",
      "training loss 0.4042169749736786\n",
      "test loss 0.6957443356513977\n",
      "training loss 0.422985702753067\n",
      "test loss 0.7011741399765015\n",
      "training loss 0.3601455092430115\n",
      "test loss 0.6894230842590332\n",
      "training loss 0.3869049847126007\n",
      "test loss 0.6852489709854126\n",
      "training loss 0.36287856101989746\n",
      "test loss 0.6838460564613342\n"
     ]
    }
   ],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.8944214519744784\n",
      "AUPRC 0.23560937087365116\n"
     ]
    }
   ],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "print(\"AUROC\", metrics.roc_auc_score(y_test,prob))\n",
    "print(\"AUPRC\", metrics.average_precision_score(y_test,prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 0.80 sec\n",
      "Memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) >> 20\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.906821709072097\n",
      "Logit AUPRC 0.29320206251427694\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "print(\"Logit AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"Logit AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost AUROC 0.881633196946182\n",
      "XGBoost AUPRC 0.23298586979956165\n"
     ]
    }
   ],
   "source": [
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "print(\"XGBoost AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"XGBoost AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf AUROC 0.8835888632836384\n",
      "rf AUPRC 0.22250687945850764\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "print(\"rf AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"rf AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm AUROC 0.7103379893085014\n",
      "svm AUPRC 0.2544622721236494\n"
     ]
    }
   ],
   "source": [
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "print(\"svm AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"svm AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 3.85 sec\n",
      "Memory usage: 1.00 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) >> 20\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. embedding dimension: 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 64\n",
    "z_np = pickle.load(open(data_path+f'hybrid_embedding_{embed_dim}_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15444, 64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.6912739872932434\n",
      "test loss 0.6688856482505798\n",
      "training loss 0.772494375705719\n",
      "test loss 0.6619194746017456\n",
      "training loss 0.7531092166900635\n",
      "test loss 0.6590011715888977\n",
      "training loss 0.7538487911224365\n",
      "test loss 0.6519534587860107\n",
      "training loss 0.6921680569648743\n",
      "test loss 0.6474665999412537\n",
      "training loss 0.6933981776237488\n",
      "test loss 0.6435495018959045\n",
      "training loss 0.6594531536102295\n",
      "test loss 0.6392725110054016\n",
      "training loss 0.6254011392593384\n",
      "test loss 0.6313424706459045\n",
      "training loss 0.5495206117630005\n",
      "test loss 0.632648766040802\n",
      "training loss 0.5254269242286682\n",
      "test loss 0.6255457997322083\n",
      "training loss 0.516959547996521\n",
      "test loss 0.621802031993866\n",
      "training loss 0.500640332698822\n",
      "test loss 0.6221750974655151\n",
      "training loss 0.6266502141952515\n",
      "test loss 0.614241898059845\n",
      "training loss 0.5432540774345398\n",
      "test loss 0.6117666959762573\n",
      "training loss 0.5194694399833679\n",
      "test loss 0.6001848578453064\n",
      "training loss 0.5238917469978333\n",
      "test loss 0.5994719862937927\n",
      "training loss 0.5337645411491394\n",
      "test loss 0.599977970123291\n",
      "training loss 0.5024670958518982\n",
      "test loss 0.5995736122131348\n",
      "training loss 0.5248032808303833\n",
      "test loss 0.5786208510398865\n",
      "training loss 0.4697600305080414\n",
      "test loss 0.5843644142150879\n",
      "training loss 0.49878162145614624\n",
      "test loss 0.5712512731552124\n",
      "training loss 0.41369688510894775\n",
      "test loss 0.5755165219306946\n",
      "training loss 0.43869754672050476\n",
      "test loss 0.5639335513114929\n",
      "training loss 0.4886225163936615\n",
      "test loss 0.5719654560089111\n",
      "training loss 0.46584922075271606\n",
      "test loss 0.558745265007019\n",
      "training loss 0.45232388377189636\n",
      "test loss 0.5595834851264954\n",
      "training loss 0.35144931077957153\n",
      "test loss 0.5525535941123962\n",
      "training loss 0.43140709400177\n",
      "test loss 0.5522462129592896\n",
      "training loss 0.41694149374961853\n",
      "test loss 0.5219733119010925\n",
      "training loss 0.4721282422542572\n",
      "test loss 0.5313018560409546\n"
     ]
    }
   ],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.8868848861088903\n",
      "AUPRC 0.24753716796759948\n"
     ]
    }
   ],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "print(\"AUROC\", metrics.roc_auc_score(y_test,prob))\n",
    "print(\"AUPRC\", metrics.average_precision_score(y_test,prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 0.65 sec\n",
      "Memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) >> 20\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.8922580695731239\n",
      "Logit AUPRC 0.24818444603187836\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "print(\"Logit AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"Logit AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost AUROC 0.8641811284077194\n",
      "XGBoost AUPRC 0.16294985552358673\n"
     ]
    }
   ],
   "source": [
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "print(\"XGBoost AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"XGBoost AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf AUROC 0.8622920879775511\n",
      "rf AUPRC 0.17087903746957997\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "print(\"rf AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"rf AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm AUROC 0.7548832870871154\n",
      "svm AUPRC 0.2276472066940597\n"
     ]
    }
   ],
   "source": [
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "print(\"svm AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"svm AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 2.34 sec\n",
      "Memory usage: -1.00 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) >> 20\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. embedding dimension: 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15444, 256)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "z_np = pickle.load(open(data_path+f'hybrid_embedding_{embed_dim}_'+exp_id+'.pkl','rb'))\n",
    "z_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.697702944278717\n",
      "test loss 0.5835279226303101\n",
      "training loss 0.47260820865631104\n",
      "test loss 0.5818967819213867\n",
      "training loss 0.4368528127670288\n",
      "test loss 0.6551796197891235\n",
      "training loss 0.45239049196243286\n",
      "test loss 0.6590988636016846\n",
      "training loss 0.3849613666534424\n",
      "test loss 0.6861411929130554\n",
      "training loss 0.45679205656051636\n",
      "test loss 0.7240675091743469\n",
      "training loss 0.5508834719657898\n",
      "test loss 0.7755852341651917\n",
      "training loss 0.4690874218940735\n",
      "test loss 0.776006281375885\n",
      "training loss 0.5040605664253235\n",
      "test loss 0.7550899386405945\n",
      "training loss 0.5205227732658386\n",
      "test loss 0.763166606426239\n",
      "training loss 0.5187536478042603\n",
      "test loss 0.8095883131027222\n",
      "training loss 0.407887727022171\n",
      "test loss 0.8267520070075989\n",
      "training loss 0.48034805059432983\n",
      "test loss 0.8083946108818054\n",
      "training loss 0.4377565085887909\n",
      "test loss 0.830143928527832\n",
      "training loss 0.4246865212917328\n",
      "test loss 0.8047390580177307\n",
      "training loss 0.40701791644096375\n",
      "test loss 0.8348546028137207\n",
      "training loss 0.38453763723373413\n",
      "test loss 0.7940856218338013\n",
      "training loss 0.39659130573272705\n",
      "test loss 0.7811952829360962\n",
      "training loss 0.34041285514831543\n",
      "test loss 0.8030744791030884\n",
      "training loss 0.371109277009964\n",
      "test loss 0.7300056219100952\n",
      "training loss 0.3820957839488983\n",
      "test loss 0.7673435211181641\n",
      "training loss 0.39489641785621643\n",
      "test loss 0.7438811659812927\n",
      "training loss 0.35589054226875305\n",
      "test loss 0.7428444027900696\n",
      "training loss 0.35123947262763977\n",
      "test loss 0.7212847471237183\n",
      "training loss 0.35515427589416504\n",
      "test loss 0.7254965901374817\n",
      "training loss 0.3578774929046631\n",
      "test loss 0.6668305993080139\n",
      "training loss 0.32905295491218567\n",
      "test loss 0.7069931030273438\n",
      "training loss 0.3158629238605499\n",
      "test loss 0.6834981441497803\n",
      "training loss 0.34633567929267883\n",
      "test loss 0.6571851968765259\n",
      "training loss 0.34585416316986084\n",
      "test loss 0.6486566066741943\n"
     ]
    }
   ],
   "source": [
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)\n",
    "\n",
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.9023812883098967\n",
      "AUPRC 0.23917922159862307\n"
     ]
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt').state_dict())\n",
    "\n",
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "print(\"AUROC\", metrics.roc_auc_score(y_test,prob))\n",
    "print(\"AUPRC\", metrics.average_precision_score(y_test,prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 1.44 sec\n",
      "Memory usage: -2.00 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) >> 20\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.911077928796501\n",
      "Logit AUPRC 0.3002561803375123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/puzhouwang/opt/anaconda3/envs/drg/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost AUROC 0.8896870933860072\n",
      "XGBoost AUPRC 0.2632081228614367\n",
      "rf AUROC 0.8816763078273684\n",
      "rf AUPRC 0.23624628977900908\n",
      "svm AUROC 0.6968795560363072\n",
      "svm AUPRC 0.25686341301210047\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "print(\"Logit AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"Logit AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "\n",
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "print(\"XGBoost AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"XGBoost AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "\n",
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "print(\"rf AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"rf AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "\n",
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "print(\"svm AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"svm AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 6.97 sec\n",
      "Memory usage: 1.00 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) >> 20\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. without bait-prey edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15444, 128)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "z_np = pickle.load(open(data_path+f'hybrid_embedding_{embed_dim}_no_bp_'+exp_id+'.pkl','rb'))\n",
    "z_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)\n",
    "\n",
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.7889140844345093\n",
      "test loss 0.6661843657493591\n",
      "training loss 0.7854001522064209\n",
      "test loss 0.6652193069458008\n",
      "training loss 0.725344717502594\n",
      "test loss 0.6541496515274048\n",
      "training loss 0.6227079629898071\n",
      "test loss 0.644943356513977\n",
      "training loss 0.5786552429199219\n",
      "test loss 0.6310686469078064\n",
      "training loss 0.5819877982139587\n",
      "test loss 0.6197298169136047\n",
      "training loss 0.5820562839508057\n",
      "test loss 0.6033995747566223\n",
      "training loss 0.5147783160209656\n",
      "test loss 0.5936700701713562\n",
      "training loss 0.42880046367645264\n",
      "test loss 0.5832734107971191\n",
      "training loss 0.47781047224998474\n",
      "test loss 0.5695552825927734\n",
      "training loss 0.4579105079174042\n",
      "test loss 0.5529550909996033\n",
      "training loss 0.4814232885837555\n",
      "test loss 0.5492339730262756\n",
      "training loss 0.47337865829467773\n",
      "test loss 0.5364567637443542\n",
      "training loss 0.39170488715171814\n",
      "test loss 0.5327391624450684\n",
      "training loss 0.49979180097579956\n",
      "test loss 0.5350932478904724\n",
      "training loss 0.3900376856327057\n",
      "test loss 0.5221314430236816\n",
      "training loss 0.4126972258090973\n",
      "test loss 0.4998723268508911\n",
      "training loss 0.3429889380931854\n",
      "test loss 0.5004804730415344\n",
      "training loss 0.3952331244945526\n",
      "test loss 0.4871029257774353\n",
      "training loss 0.3898022770881653\n",
      "test loss 0.483925998210907\n",
      "training loss 0.38100844621658325\n",
      "test loss 0.4749983251094818\n",
      "training loss 0.3265349566936493\n",
      "test loss 0.46853068470954895\n",
      "training loss 0.42342516779899597\n",
      "test loss 0.4693150520324707\n",
      "training loss 0.3935706615447998\n",
      "test loss 0.4658990800380707\n",
      "training loss 0.38178542256355286\n",
      "test loss 0.47943803668022156\n",
      "training loss 0.41257989406585693\n",
      "test loss 0.4368772506713867\n",
      "training loss 0.3853570222854614\n",
      "test loss 0.43121930956840515\n",
      "training loss 0.4112556278705597\n",
      "test loss 0.4619125425815582\n",
      "training loss 0.374042272567749\n",
      "test loss 0.42054232954978943\n",
      "training loss 0.3708077371120453\n",
      "test loss 0.4378323256969452\n"
     ]
    }
   ],
   "source": [
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)\n",
    "\n",
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+f'nn_clf_hybrid_embedding_{embed_dim}_no_bp_.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.8532623179545062\n",
      "AUPRC 0.1293745920051117\n"
     ]
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+f'nn_clf_hybrid_embedding_{embed_dim}_no_bp_.pt').state_dict())\n",
    "\n",
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "print(\"AUROC\", metrics.roc_auc_score(y_test,prob))\n",
    "print(\"AUPRC\", metrics.average_precision_score(y_test,prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 0.80 sec\n",
      "Memory usage: -2.00 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) >> 20\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.8650355076894136\n",
      "Logit AUPRC 0.17650556970300618\n",
      "XGBoost AUROC 0.8382793271567197\n",
      "XGBoost AUPRC 0.11757145843004146\n",
      "rf AUROC 0.8207880669080876\n",
      "rf AUPRC 0.095345092417475\n",
      "svm AUROC 0.7255600495383216\n",
      "svm AUPRC 0.1003038941858655\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "print(\"Logit AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"Logit AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "\n",
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "print(\"XGBoost AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"XGBoost AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "\n",
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "print(\"rf AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"rf AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "\n",
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "print(\"svm AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"svm AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 4.08 sec\n",
      "Memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) >> 20\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drg",
   "language": "python",
   "name": "drg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
